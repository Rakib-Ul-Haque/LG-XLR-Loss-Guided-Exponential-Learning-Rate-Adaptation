{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6561a91c-627b-40b2-85ba-f040da794833",
   "metadata": {},
   "source": [
    "<h5>This code is part of XLR. It implements exponential learning rate decay model training. Date: 01/09/2025</h5>\n",
    "\n",
    "<h5>Contact: rakibul.haque@utsa.edu</h5>\n",
    "\n",
    "<h5>Cite as: R. U. Haque and P. Markopoulos,\"XLR: A Universal Framework for Learning rate Adaptation via Exponential Range Exploration\", 2025</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14bea459-a295-4223-aefe-cff4c940855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from collections import Counter\n",
    "import random\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "import queue\n",
    "from collections import deque\n",
    "from torch import Tensor\n",
    "from typing import Type\n",
    "import torchvision.models as models\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fd80a1c-6ef3-4179-8a3b-d2579f76dc1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch 1, LR: 0.010001\n",
      "[1, 100] loss: 2.295 | Train acc: 12.3%\n",
      "[1, 200] loss: 2.104 | Train acc: 17.5%\n",
      "[1, 300] loss: 1.995 | Train acc: 20.2%\n",
      "Test Epoch: 1, Loss: 1.6998, Acc: 39.55%\n",
      "Epoch 2, LR: 0.010002\n",
      "[2, 100] loss: 1.787 | Train acc: 33.7%\n",
      "[2, 200] loss: 1.702 | Train acc: 35.3%\n",
      "[2, 300] loss: 1.644 | Train acc: 36.7%\n",
      "Test Epoch: 2, Loss: 1.4240, Acc: 46.89%\n",
      "Epoch 3, LR: 0.010003\n",
      "[3, 100] loss: 1.560 | Train acc: 42.5%\n",
      "[3, 200] loss: 1.527 | Train acc: 43.1%\n",
      "[3, 300] loss: 1.518 | Train acc: 43.5%\n",
      "Test Epoch: 3, Loss: 1.2835, Acc: 54.07%\n",
      "Epoch 4, LR: 0.010004\n",
      "[4, 100] loss: 1.421 | Train acc: 48.2%\n",
      "[4, 200] loss: 1.412 | Train acc: 48.5%\n",
      "[4, 300] loss: 1.392 | Train acc: 48.8%\n",
      "Test Epoch: 4, Loss: 1.1638, Acc: 58.09%\n",
      "Epoch 5, LR: 0.010005\n",
      "[5, 100] loss: 1.313 | Train acc: 52.3%\n",
      "[5, 200] loss: 1.286 | Train acc: 52.7%\n",
      "[5, 300] loss: 1.289 | Train acc: 52.9%\n",
      "Test Epoch: 5, Loss: 1.0708, Acc: 61.71%\n",
      "Epoch 6, LR: 0.010006\n",
      "[6, 100] loss: 1.229 | Train acc: 55.7%\n",
      "[6, 200] loss: 1.196 | Train acc: 56.1%\n",
      "[6, 300] loss: 1.194 | Train acc: 56.4%\n",
      "Test Epoch: 6, Loss: 0.9736, Acc: 65.01%\n",
      "Epoch 7, LR: 0.010007\n",
      "[7, 100] loss: 1.134 | Train acc: 59.9%\n",
      "[7, 200] loss: 1.126 | Train acc: 59.6%\n",
      "[7, 300] loss: 1.129 | Train acc: 59.5%\n",
      "Test Epoch: 7, Loss: 0.9557, Acc: 66.23%\n",
      "Epoch 8, LR: 0.010008\n",
      "[8, 100] loss: 1.072 | Train acc: 61.4%\n",
      "[8, 200] loss: 1.061 | Train acc: 61.6%\n",
      "[8, 300] loss: 1.056 | Train acc: 61.8%\n",
      "Test Epoch: 8, Loss: 0.8646, Acc: 69.45%\n",
      "Epoch 9, LR: 0.010009\n",
      "[9, 100] loss: 1.026 | Train acc: 63.9%\n",
      "[9, 200] loss: 1.006 | Train acc: 64.2%\n",
      "[9, 300] loss: 1.016 | Train acc: 64.1%\n",
      "Test Epoch: 9, Loss: 0.8245, Acc: 70.91%\n",
      "Epoch 10, LR: 0.010010\n",
      "[10, 100] loss: 0.978 | Train acc: 65.4%\n",
      "[10, 200] loss: 0.962 | Train acc: 65.3%\n",
      "[10, 300] loss: 0.953 | Train acc: 65.7%\n",
      "Test Epoch: 10, Loss: 0.7760, Acc: 72.35%\n",
      "Epoch 11, LR: 0.010011\n",
      "[11, 100] loss: 0.939 | Train acc: 66.9%\n",
      "[11, 200] loss: 0.927 | Train acc: 67.0%\n",
      "[11, 300] loss: 0.924 | Train acc: 67.1%\n",
      "Test Epoch: 11, Loss: 0.7353, Acc: 73.63%\n",
      "Epoch 12, LR: 0.010012\n",
      "[12, 100] loss: 0.895 | Train acc: 68.2%\n",
      "[12, 200] loss: 0.892 | Train acc: 68.3%\n",
      "[12, 300] loss: 0.886 | Train acc: 68.3%\n",
      "Test Epoch: 12, Loss: 0.7038, Acc: 75.15%\n",
      "Epoch 13, LR: 0.010013\n",
      "[13, 100] loss: 0.861 | Train acc: 69.5%\n",
      "[13, 200] loss: 0.865 | Train acc: 69.4%\n",
      "[13, 300] loss: 0.860 | Train acc: 69.5%\n",
      "Test Epoch: 13, Loss: 0.6755, Acc: 76.55%\n",
      "Epoch 14, LR: 0.010014\n",
      "[14, 100] loss: 0.840 | Train acc: 70.4%\n",
      "[14, 200] loss: 0.840 | Train acc: 70.2%\n",
      "[14, 300] loss: 0.842 | Train acc: 70.2%\n",
      "Test Epoch: 14, Loss: 0.6546, Acc: 77.16%\n",
      "Epoch 15, LR: 0.010015\n",
      "[15, 100] loss: 0.816 | Train acc: 71.4%\n",
      "[15, 200] loss: 0.823 | Train acc: 71.2%\n",
      "[15, 300] loss: 0.820 | Train acc: 71.3%\n",
      "Test Epoch: 15, Loss: 0.6499, Acc: 77.03%\n",
      "Epoch 16, LR: 0.010016\n",
      "[16, 100] loss: 0.797 | Train acc: 72.2%\n",
      "[16, 200] loss: 0.812 | Train acc: 71.7%\n",
      "[16, 300] loss: 0.785 | Train acc: 72.1%\n",
      "Test Epoch: 16, Loss: 0.6357, Acc: 78.19%\n",
      "Epoch 17, LR: 0.010017\n",
      "[17, 100] loss: 0.788 | Train acc: 72.4%\n",
      "[17, 200] loss: 0.778 | Train acc: 72.5%\n",
      "[17, 300] loss: 0.781 | Train acc: 72.6%\n",
      "Test Epoch: 17, Loss: 0.6149, Acc: 78.72%\n",
      "Epoch 18, LR: 0.010018\n",
      "[18, 100] loss: 0.765 | Train acc: 73.0%\n",
      "[18, 200] loss: 0.756 | Train acc: 73.2%\n",
      "[18, 300] loss: 0.756 | Train acc: 73.2%\n",
      "Test Epoch: 18, Loss: 0.6251, Acc: 78.71%\n",
      "Epoch 19, LR: 0.010019\n",
      "[19, 100] loss: 0.750 | Train acc: 73.5%\n",
      "[19, 200] loss: 0.750 | Train acc: 73.8%\n",
      "[19, 300] loss: 0.750 | Train acc: 73.8%\n",
      "Test Epoch: 19, Loss: 0.6128, Acc: 79.07%\n",
      "Epoch 20, LR: 0.010020\n",
      "[20, 100] loss: 0.734 | Train acc: 74.5%\n",
      "[20, 200] loss: 0.737 | Train acc: 74.4%\n",
      "[20, 300] loss: 0.728 | Train acc: 74.5%\n",
      "Test Epoch: 20, Loss: 0.5909, Acc: 79.64%\n",
      "Epoch 21, LR: 0.010021\n",
      "[21, 100] loss: 0.718 | Train acc: 75.0%\n",
      "[21, 200] loss: 0.727 | Train acc: 74.8%\n",
      "[21, 300] loss: 0.713 | Train acc: 74.8%\n",
      "Test Epoch: 21, Loss: 0.5682, Acc: 80.76%\n",
      "Epoch 22, LR: 0.010022\n",
      "[22, 100] loss: 0.708 | Train acc: 74.9%\n",
      "[22, 200] loss: 0.714 | Train acc: 75.0%\n",
      "[22, 300] loss: 0.723 | Train acc: 74.9%\n",
      "Test Epoch: 22, Loss: 0.5771, Acc: 79.89%\n",
      "Epoch 23, LR: 0.010023\n",
      "[23, 100] loss: 0.697 | Train acc: 75.8%\n",
      "[23, 200] loss: 0.704 | Train acc: 75.6%\n",
      "[23, 300] loss: 0.703 | Train acc: 75.5%\n",
      "Test Epoch: 23, Loss: 0.5425, Acc: 81.75%\n",
      "Epoch 24, LR: 0.010024\n",
      "[24, 100] loss: 0.682 | Train acc: 76.3%\n",
      "[24, 200] loss: 0.692 | Train acc: 76.1%\n",
      "[24, 300] loss: 0.691 | Train acc: 76.0%\n",
      "Test Epoch: 24, Loss: 0.5449, Acc: 81.68%\n",
      "Epoch 25, LR: 0.010025\n",
      "[25, 100] loss: 0.688 | Train acc: 75.8%\n",
      "[25, 200] loss: 0.694 | Train acc: 75.7%\n",
      "[25, 300] loss: 0.675 | Train acc: 76.1%\n",
      "Test Epoch: 25, Loss: 0.5317, Acc: 82.05%\n",
      "Epoch 26, LR: 0.010026\n",
      "[26, 100] loss: 0.655 | Train acc: 77.0%\n",
      "[26, 200] loss: 0.676 | Train acc: 76.8%\n",
      "[26, 300] loss: 0.679 | Train acc: 76.7%\n",
      "Test Epoch: 26, Loss: 0.5248, Acc: 81.96%\n",
      "Epoch 27, LR: 0.010027\n",
      "[27, 100] loss: 0.662 | Train acc: 76.9%\n",
      "[27, 200] loss: 0.660 | Train acc: 76.8%\n",
      "[27, 300] loss: 0.659 | Train acc: 76.9%\n",
      "Test Epoch: 27, Loss: 0.5319, Acc: 82.00%\n",
      "Epoch 28, LR: 0.010028\n",
      "[28, 100] loss: 0.658 | Train acc: 76.6%\n",
      "[28, 200] loss: 0.660 | Train acc: 76.8%\n",
      "[28, 300] loss: 0.654 | Train acc: 77.0%\n",
      "Test Epoch: 28, Loss: 0.5330, Acc: 81.65%\n",
      "Epoch 29, LR: 0.010029\n",
      "[29, 100] loss: 0.654 | Train acc: 76.7%\n",
      "[29, 200] loss: 0.651 | Train acc: 77.1%\n",
      "[29, 300] loss: 0.658 | Train acc: 77.1%\n",
      "Test Epoch: 29, Loss: 0.5311, Acc: 82.04%\n",
      "Epoch 30, LR: 0.010030\n",
      "[30, 100] loss: 0.635 | Train acc: 77.8%\n",
      "[30, 200] loss: 0.642 | Train acc: 77.6%\n",
      "[30, 300] loss: 0.650 | Train acc: 77.5%\n",
      "Test Epoch: 30, Loss: 0.5065, Acc: 83.13%\n",
      "Epoch 31, LR: 0.010031\n",
      "[31, 100] loss: 0.630 | Train acc: 78.1%\n",
      "[31, 200] loss: 0.631 | Train acc: 77.9%\n",
      "[31, 300] loss: 0.630 | Train acc: 77.9%\n",
      "Test Epoch: 31, Loss: 0.5133, Acc: 82.51%\n",
      "Epoch 32, LR: 0.010032\n",
      "[32, 100] loss: 0.628 | Train acc: 78.4%\n",
      "[32, 200] loss: 0.639 | Train acc: 78.1%\n",
      "[32, 300] loss: 0.620 | Train acc: 78.2%\n",
      "Test Epoch: 32, Loss: 0.4936, Acc: 82.96%\n",
      "Epoch 33, LR: 0.010033\n",
      "[33, 100] loss: 0.622 | Train acc: 78.6%\n",
      "[33, 200] loss: 0.615 | Train acc: 78.7%\n",
      "[33, 300] loss: 0.623 | Train acc: 78.6%\n",
      "Test Epoch: 33, Loss: 0.4884, Acc: 83.35%\n",
      "Epoch 34, LR: 0.010034\n",
      "[34, 100] loss: 0.615 | Train acc: 78.6%\n",
      "[34, 200] loss: 0.619 | Train acc: 78.2%\n",
      "[34, 300] loss: 0.618 | Train acc: 78.4%\n",
      "Test Epoch: 34, Loss: 0.4877, Acc: 83.41%\n",
      "Epoch 35, LR: 0.010035\n",
      "[35, 100] loss: 0.605 | Train acc: 78.9%\n",
      "[35, 200] loss: 0.620 | Train acc: 78.6%\n",
      "[35, 300] loss: 0.592 | Train acc: 78.6%\n",
      "Test Epoch: 35, Loss: 0.4844, Acc: 84.00%\n",
      "Epoch 36, LR: 0.010036\n",
      "[36, 100] loss: 0.601 | Train acc: 79.1%\n",
      "[36, 200] loss: 0.608 | Train acc: 78.9%\n",
      "[36, 300] loss: 0.606 | Train acc: 79.0%\n",
      "Test Epoch: 36, Loss: 0.4728, Acc: 84.28%\n",
      "Epoch 37, LR: 0.010037\n",
      "[37, 100] loss: 0.597 | Train acc: 79.4%\n",
      "[37, 200] loss: 0.600 | Train acc: 79.3%\n",
      "[37, 300] loss: 0.599 | Train acc: 79.1%\n",
      "Test Epoch: 37, Loss: 0.4902, Acc: 83.21%\n",
      "Epoch 38, LR: 0.010038\n",
      "[38, 100] loss: 0.602 | Train acc: 79.0%\n",
      "[38, 200] loss: 0.598 | Train acc: 79.2%\n",
      "[38, 300] loss: 0.596 | Train acc: 79.2%\n",
      "Test Epoch: 38, Loss: 0.4668, Acc: 84.31%\n",
      "Epoch 39, LR: 0.010039\n",
      "[39, 100] loss: 0.585 | Train acc: 79.7%\n",
      "[39, 200] loss: 0.597 | Train acc: 79.4%\n",
      "[39, 300] loss: 0.596 | Train acc: 79.3%\n",
      "Test Epoch: 39, Loss: 0.4755, Acc: 84.09%\n",
      "Epoch 40, LR: 0.010040\n",
      "[40, 100] loss: 0.582 | Train acc: 79.9%\n",
      "[40, 200] loss: 0.599 | Train acc: 79.5%\n",
      "[40, 300] loss: 0.592 | Train acc: 79.4%\n",
      "Test Epoch: 40, Loss: 0.4667, Acc: 84.57%\n",
      "Epoch 41, LR: 0.010041\n",
      "[41, 100] loss: 0.578 | Train acc: 80.0%\n",
      "[41, 200] loss: 0.592 | Train acc: 79.7%\n",
      "[41, 300] loss: 0.574 | Train acc: 79.7%\n",
      "Test Epoch: 41, Loss: 0.4546, Acc: 85.09%\n",
      "Epoch 42, LR: 0.010042\n",
      "[42, 100] loss: 0.584 | Train acc: 79.9%\n",
      "[42, 200] loss: 0.584 | Train acc: 79.8%\n",
      "[42, 300] loss: 0.571 | Train acc: 80.0%\n",
      "Test Epoch: 42, Loss: 0.4494, Acc: 84.95%\n",
      "Epoch 43, LR: 0.010043\n",
      "[43, 100] loss: 0.558 | Train acc: 80.5%\n",
      "[43, 200] loss: 0.575 | Train acc: 80.3%\n",
      "[43, 300] loss: 0.568 | Train acc: 80.2%\n",
      "Test Epoch: 43, Loss: 0.4505, Acc: 85.03%\n",
      "Epoch 44, LR: 0.010044\n",
      "[44, 100] loss: 0.578 | Train acc: 79.4%\n",
      "[44, 200] loss: 0.570 | Train acc: 79.6%\n",
      "[44, 300] loss: 0.578 | Train acc: 79.7%\n",
      "Test Epoch: 44, Loss: 0.4570, Acc: 84.52%\n",
      "Epoch 45, LR: 0.010045\n",
      "[45, 100] loss: 0.548 | Train acc: 80.4%\n",
      "[45, 200] loss: 0.572 | Train acc: 80.2%\n",
      "[45, 300] loss: 0.561 | Train acc: 80.3%\n",
      "Test Epoch: 45, Loss: 0.4518, Acc: 84.68%\n",
      "Epoch 46, LR: 0.010046\n",
      "[46, 100] loss: 0.559 | Train acc: 80.2%\n",
      "[46, 200] loss: 0.565 | Train acc: 80.2%\n",
      "[46, 300] loss: 0.561 | Train acc: 80.4%\n",
      "Test Epoch: 46, Loss: 0.4505, Acc: 84.97%\n",
      "Epoch 47, LR: 0.010047\n",
      "[47, 100] loss: 0.553 | Train acc: 80.5%\n",
      "[47, 200] loss: 0.562 | Train acc: 80.4%\n",
      "[47, 300] loss: 0.553 | Train acc: 80.6%\n",
      "Test Epoch: 47, Loss: 0.4511, Acc: 85.00%\n",
      "Epoch 48, LR: 0.010048\n",
      "[48, 100] loss: 0.560 | Train acc: 80.6%\n",
      "[48, 200] loss: 0.561 | Train acc: 80.5%\n",
      "[48, 300] loss: 0.557 | Train acc: 80.5%\n",
      "Test Epoch: 48, Loss: 0.4421, Acc: 85.12%\n",
      "Epoch 49, LR: 0.010049\n",
      "[49, 100] loss: 0.548 | Train acc: 80.7%\n",
      "[49, 200] loss: 0.555 | Train acc: 80.6%\n",
      "[49, 300] loss: 0.553 | Train acc: 80.5%\n",
      "Test Epoch: 49, Loss: 0.4341, Acc: 85.32%\n",
      "Epoch 50, LR: 0.010050\n",
      "[50, 100] loss: 0.541 | Train acc: 80.7%\n",
      "[50, 200] loss: 0.562 | Train acc: 80.5%\n",
      "[50, 300] loss: 0.555 | Train acc: 80.7%\n",
      "Test Epoch: 50, Loss: 0.4357, Acc: 85.67%\n",
      "Best test accuracy: 85.67%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "initial_lr = 0.01#0.00001 #0.01\n",
    "weight_decay = 0.0005\n",
    "momentum = 0.9\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "phase_boundaries = [80, 120]  # Epochs where LR would normally be reduced by 10x\n",
    "\n",
    "\n",
    "# Define transformations for data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_dataset = torchvision.datasets.CIFAR10(\n",
    "#     root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "\n",
    "subset_dataset = torch.load('val_dataset.pth',weights_only=False)\n",
    "remaining_dataset = torch.load('test_dataset.pth',weights_only=False)\n",
    "val_loader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(remaining_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 10)  # Change output to 10 for CIFAR-10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu5(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = cnn().to(device)\n",
    "file_path = \"s_cnn.pth\"\n",
    "initial_weights=torch.load(file_path, weights_only=True)\n",
    "# Print(\"Model's initial weights\", initial_weights)\n",
    "model.load_state_dict(initial_weights)\n",
    "\n",
    "# Custom LR scheduler implementing the exponential schedule\n",
    "class TaperedExponentialLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, initial_lr, weight_decay, momentum, phase_boundaries, last_epoch=-1):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "        self.phase_boundaries = phase_boundaries\n",
    "        self.current_phase = 0\n",
    "        self.alphas = []\n",
    "        self.compute_alphas()\n",
    "        super(TaperedExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def compute_alpha(self, lr):\n",
    "        a = 1\n",
    "        b = -(1 + self.momentum - self.weight_decay * lr)\n",
    "        c = self.momentum\n",
    "        discriminant = b**2 - 4*a*c\n",
    "        return (-b + math.sqrt(discriminant)) / (2*a)\n",
    "\n",
    "    def compute_alphas(self):\n",
    "        self.alphas = []\n",
    "        for i in range(len(self.phase_boundaries) + 1):\n",
    "            phase_lr = self.initial_lr * (0.1 ** i)\n",
    "            self.alphas.append(self.compute_alpha(phase_lr))\n",
    "\n",
    "    def get_lr(self):\n",
    "        epoch = self.last_epoch\n",
    "        # Determine current phase\n",
    "        self.current_phase = 0\n",
    "        for i, boundary in enumerate(self.phase_boundaries):\n",
    "            if epoch >= boundary:\n",
    "                self.current_phase = i + 1\n",
    "        \n",
    "        # Compute base LR for phase\n",
    "        if self.current_phase == 0:\n",
    "            base_lr = self.initial_lr * (self.alphas[0] ** -1)\n",
    "        else:\n",
    "            prev_phase_lr = self.initial_lr * (0.1 ** (self.current_phase - 1))\n",
    "            base_lr = self.initial_lr * (self.alphas[self.current_phase] ** -1) * \\\n",
    "                     (self.alphas[self.current_phase - 1] ** -1) * (0.1 ** self.current_phase)\n",
    "        \n",
    "        # Compute epochs in current phase\n",
    "        prev_boundary = self.phase_boundaries[self.current_phase - 1] if self.current_phase > 0 else 0\n",
    "        epochs_in_phase = epoch - prev_boundary\n",
    "        \n",
    "        current_lr = base_lr * (self.alphas[self.current_phase] ** (-2 * epochs_in_phase))\n",
    "        return [current_lr for _ in self.base_lrs]\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum, weight_decay=0)  # No weight decay!\n",
    "scheduler = TaperedExponentialLR(optimizer, initial_lr, weight_decay, momentum, phase_boundaries)\n",
    "\n",
    "# Training loop\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, LR: {scheduler.get_lr()[0]:.6f}')\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 99:\n",
    "            print(f'[{epoch+1}, {batch_idx+1}] loss: {running_loss/100:.3f} | Train acc: {100.*correct/total:.1f}%')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    acc = 100. * correct / total\n",
    "    print(f'Test Epoch: {epoch+1}, Loss: {test_loss/(batch_idx+1):.4f}, Acc: {acc:.2f}%')\n",
    "    return acc\n",
    "\n",
    "best_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    train(epoch)\n",
    "    acc = test(epoch)\n",
    "    \n",
    "    # Save best model\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "print(f'Best test accuracy: {best_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447a59d-5704-40b3-842b-4cc471e524cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
