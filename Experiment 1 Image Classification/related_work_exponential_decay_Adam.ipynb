{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6561a91c-627b-40b2-85ba-f040da794833",
   "metadata": {},
   "source": [
    "<h5>This code is part of XLR. It implements exponential learning rate decay model training. Date: 01/09/2025</h5>\n",
    "\n",
    "<h5>Contact: rakibul.haque@utsa.edu</h5>\n",
    "\n",
    "<h5>Cite as: R. U. Haque and P. Markopoulos,\"XLR: A Universal Framework for Learning rate Adaptation via Exponential Range Exploration\", 2025</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14bea459-a295-4223-aefe-cff4c940855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from collections import Counter\n",
    "import random\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "import queue\n",
    "from collections import deque\n",
    "from torch import Tensor\n",
    "from typing import Type\n",
    "import torchvision.models as models\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd80a1c-6ef3-4179-8a3b-d2579f76dc1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch 1, LR: 0.010000\n",
      "[1, 100] loss: 2.579 | acc: 13.5%\n",
      "[1, 200] loss: 2.099 | acc: 17.4%\n",
      "[1, 300] loss: 2.053 | acc: 19.2%\n",
      "Test Epoch: 1, Loss: 1.9471, Acc: 25.88%\n",
      "Epoch 2, LR: 0.010000\n",
      "[2, 100] loss: 2.012 | acc: 23.9%\n",
      "[2, 200] loss: 2.000 | acc: 23.8%\n",
      "[2, 300] loss: 1.976 | acc: 23.9%\n",
      "Test Epoch: 2, Loss: 1.8910, Acc: 27.87%\n",
      "Epoch 3, LR: 0.010000\n",
      "[3, 100] loss: 1.947 | acc: 25.3%\n",
      "[3, 200] loss: 1.933 | acc: 25.2%\n",
      "[3, 300] loss: 1.928 | acc: 25.3%\n",
      "Test Epoch: 3, Loss: 1.8444, Acc: 29.20%\n",
      "Epoch 4, LR: 0.010000\n",
      "[4, 100] loss: 1.927 | acc: 25.9%\n",
      "[4, 200] loss: 1.912 | acc: 25.9%\n",
      "[4, 300] loss: 1.921 | acc: 25.6%\n",
      "Test Epoch: 4, Loss: 1.8375, Acc: 28.87%\n",
      "Epoch 5, LR: 0.010000\n",
      "[5, 100] loss: 1.914 | acc: 26.6%\n",
      "[5, 200] loss: 1.900 | acc: 26.2%\n",
      "[5, 300] loss: 1.915 | acc: 26.1%\n",
      "Test Epoch: 5, Loss: 1.8382, Acc: 29.05%\n",
      "Epoch 6, LR: 0.010000\n",
      "[6, 100] loss: 1.885 | acc: 26.7%\n",
      "[6, 200] loss: 1.908 | acc: 26.6%\n",
      "[6, 300] loss: 1.905 | acc: 26.1%\n",
      "Test Epoch: 6, Loss: 1.8502, Acc: 29.37%\n",
      "Epoch 7, LR: 0.010000\n",
      "[7, 100] loss: 1.894 | acc: 27.1%\n",
      "[7, 200] loss: 1.888 | acc: 26.8%\n",
      "[7, 300] loss: 1.903 | acc: 26.4%\n",
      "Test Epoch: 7, Loss: 1.8665, Acc: 28.29%\n",
      "Epoch 8, LR: 0.010000\n",
      "[8, 100] loss: 1.895 | acc: 26.1%\n",
      "[8, 200] loss: 1.889 | acc: 26.5%\n",
      "[8, 300] loss: 1.888 | acc: 26.3%\n",
      "Test Epoch: 8, Loss: 1.7956, Acc: 30.32%\n",
      "Epoch 9, LR: 0.010000\n",
      "[9, 100] loss: 1.876 | acc: 27.3%\n",
      "[9, 200] loss: 1.892 | acc: 26.7%\n",
      "[9, 300] loss: 1.895 | acc: 26.6%\n",
      "Test Epoch: 9, Loss: 1.8214, Acc: 29.52%\n",
      "Epoch 10, LR: 0.010000\n",
      "[10, 100] loss: 1.886 | acc: 26.8%\n",
      "[10, 200] loss: 1.879 | acc: 27.0%\n",
      "[10, 300] loss: 1.903 | acc: 26.7%\n",
      "Test Epoch: 10, Loss: 1.7868, Acc: 30.97%\n",
      "Epoch 11, LR: 0.010000\n",
      "[11, 100] loss: 1.876 | acc: 26.6%\n",
      "[11, 200] loss: 1.881 | acc: 26.6%\n",
      "[11, 300] loss: 1.885 | acc: 26.5%\n",
      "Test Epoch: 11, Loss: 1.8007, Acc: 30.95%\n",
      "Epoch 12, LR: 0.010000\n",
      "[12, 100] loss: 1.870 | acc: 27.7%\n",
      "[12, 200] loss: 1.873 | acc: 27.3%\n",
      "[12, 300] loss: 1.888 | acc: 26.8%\n",
      "Test Epoch: 12, Loss: 1.8615, Acc: 28.92%\n",
      "Epoch 13, LR: 0.010000\n",
      "[13, 100] loss: 1.874 | acc: 27.9%\n",
      "[13, 200] loss: 1.877 | acc: 27.6%\n",
      "[13, 300] loss: 1.884 | acc: 27.0%\n",
      "Test Epoch: 13, Loss: 1.8048, Acc: 30.23%\n",
      "Epoch 14, LR: 0.010000\n",
      "[14, 100] loss: 1.864 | acc: 27.2%\n",
      "[14, 200] loss: 1.881 | acc: 27.2%\n",
      "[14, 300] loss: 1.868 | acc: 27.1%\n",
      "Test Epoch: 14, Loss: 1.8539, Acc: 28.71%\n",
      "Epoch 15, LR: 0.010000\n",
      "[15, 100] loss: 1.866 | acc: 27.4%\n",
      "[15, 200] loss: 1.867 | acc: 27.4%\n",
      "[15, 300] loss: 1.873 | acc: 27.2%\n",
      "Test Epoch: 15, Loss: 1.8608, Acc: 29.81%\n",
      "Epoch 16, LR: 0.010000\n",
      "[16, 100] loss: 1.883 | acc: 26.4%\n",
      "[16, 200] loss: 1.866 | acc: 26.5%\n",
      "[16, 300] loss: 1.881 | acc: 26.5%\n",
      "Test Epoch: 16, Loss: 1.7770, Acc: 32.17%\n",
      "Epoch 17, LR: 0.010000\n",
      "[17, 100] loss: 1.876 | acc: 27.5%\n",
      "[17, 200] loss: 1.865 | acc: 27.5%\n",
      "[17, 300] loss: 1.868 | acc: 27.3%\n",
      "Test Epoch: 17, Loss: 1.8397, Acc: 29.73%\n",
      "Epoch 18, LR: 0.010000\n",
      "[18, 100] loss: 1.860 | acc: 27.9%\n",
      "[18, 200] loss: 1.865 | acc: 27.9%\n",
      "[18, 300] loss: 1.865 | acc: 27.5%\n",
      "Test Epoch: 18, Loss: 1.8244, Acc: 30.81%\n",
      "Epoch 19, LR: 0.010000\n",
      "[19, 100] loss: 1.867 | acc: 27.6%\n",
      "[19, 200] loss: 1.856 | acc: 27.6%\n",
      "[19, 300] loss: 1.873 | acc: 27.3%\n",
      "Test Epoch: 19, Loss: 1.8633, Acc: 29.41%\n",
      "Epoch 20, LR: 0.010000\n",
      "[20, 100] loss: 1.859 | acc: 27.6%\n",
      "[20, 200] loss: 1.850 | acc: 27.8%\n",
      "[20, 300] loss: 1.885 | acc: 27.5%\n",
      "Test Epoch: 20, Loss: 1.8691, Acc: 29.11%\n",
      "Epoch 21, LR: 0.010000\n",
      "[21, 100] loss: 1.859 | acc: 27.9%\n",
      "[21, 200] loss: 1.850 | acc: 28.1%\n",
      "[21, 300] loss: 1.929 | acc: 27.3%\n",
      "Test Epoch: 21, Loss: 1.8571, Acc: 30.35%\n",
      "Epoch 22, LR: 0.010000\n",
      "[22, 100] loss: 1.877 | acc: 27.2%\n",
      "[22, 200] loss: 1.871 | acc: 27.1%\n",
      "[22, 300] loss: 1.867 | acc: 26.9%\n",
      "Test Epoch: 22, Loss: 1.8424, Acc: 30.04%\n",
      "Epoch 23, LR: 0.010000\n",
      "[23, 100] loss: 1.865 | acc: 27.0%\n",
      "[23, 200] loss: 1.870 | acc: 27.2%\n",
      "[23, 300] loss: 1.877 | acc: 26.9%\n",
      "Test Epoch: 23, Loss: 1.9524, Acc: 26.85%\n",
      "Epoch 24, LR: 0.010000\n",
      "[24, 100] loss: 1.877 | acc: 27.4%\n",
      "[24, 200] loss: 1.861 | acc: 27.6%\n",
      "[24, 300] loss: 1.867 | acc: 27.5%\n",
      "Test Epoch: 24, Loss: 1.8612, Acc: 29.72%\n",
      "Epoch 25, LR: 0.010000\n",
      "[25, 100] loss: 1.860 | acc: 27.4%\n",
      "[25, 200] loss: 1.870 | acc: 27.2%\n",
      "[25, 300] loss: 1.875 | acc: 26.9%\n",
      "Test Epoch: 25, Loss: 1.8058, Acc: 30.17%\n",
      "Epoch 26, LR: 0.010000\n",
      "[26, 100] loss: 1.891 | acc: 26.9%\n",
      "[26, 200] loss: 1.864 | acc: 27.5%\n",
      "[26, 300] loss: 1.862 | acc: 27.4%\n",
      "Test Epoch: 26, Loss: 1.8243, Acc: 30.16%\n",
      "Epoch 27, LR: 0.010000\n",
      "[27, 100] loss: 1.874 | acc: 27.5%\n",
      "[27, 200] loss: 1.865 | acc: 27.6%\n",
      "[27, 300] loss: 1.877 | acc: 27.3%\n",
      "Test Epoch: 27, Loss: 1.8067, Acc: 30.56%\n",
      "Epoch 28, LR: 0.010000\n",
      "[28, 100] loss: 1.862 | acc: 27.7%\n",
      "[28, 200] loss: 1.860 | acc: 27.7%\n",
      "[28, 300] loss: 1.871 | acc: 27.4%\n",
      "Test Epoch: 28, Loss: 1.7864, Acc: 31.28%\n",
      "Epoch 29, LR: 0.010000\n",
      "[29, 100] loss: 1.857 | acc: 27.8%\n",
      "[29, 200] loss: 1.852 | acc: 27.8%\n",
      "[29, 300] loss: 1.864 | acc: 27.7%\n",
      "Test Epoch: 29, Loss: 1.7843, Acc: 31.43%\n",
      "Epoch 30, LR: 0.010000\n",
      "[30, 100] loss: 1.859 | acc: 27.8%\n",
      "[30, 200] loss: 1.862 | acc: 27.6%\n",
      "[30, 300] loss: 1.875 | acc: 27.2%\n",
      "Test Epoch: 30, Loss: 1.8487, Acc: 29.92%\n",
      "Epoch 31, LR: 0.010000\n",
      "[31, 100] loss: 1.866 | acc: 27.4%\n",
      "[31, 200] loss: 1.878 | acc: 27.3%\n",
      "[31, 300] loss: 1.860 | acc: 27.1%\n",
      "Test Epoch: 31, Loss: 1.8437, Acc: 29.92%\n",
      "Epoch 32, LR: 0.010000\n",
      "[32, 100] loss: 1.853 | acc: 28.6%\n",
      "[32, 200] loss: 1.875 | acc: 27.8%\n",
      "[32, 300] loss: 1.863 | acc: 27.7%\n",
      "Test Epoch: 32, Loss: 1.7976, Acc: 29.96%\n",
      "Epoch 33, LR: 0.010000\n",
      "[33, 100] loss: 1.869 | acc: 27.0%\n",
      "[33, 200] loss: 1.849 | acc: 27.7%\n",
      "[33, 300] loss: 1.866 | acc: 27.5%\n",
      "Test Epoch: 33, Loss: 1.8111, Acc: 30.59%\n",
      "Epoch 34, LR: 0.010000\n",
      "[34, 100] loss: 1.854 | acc: 28.0%\n",
      "[34, 200] loss: 1.852 | acc: 28.0%\n",
      "[34, 300] loss: 1.890 | acc: 27.7%\n",
      "Test Epoch: 34, Loss: 1.8078, Acc: 30.79%\n",
      "Epoch 35, LR: 0.010000\n",
      "[35, 100] loss: 1.861 | acc: 28.2%\n",
      "[35, 200] loss: 1.849 | acc: 28.1%\n",
      "[35, 300] loss: 1.857 | acc: 27.9%\n",
      "Test Epoch: 35, Loss: 1.8238, Acc: 31.12%\n",
      "Epoch 36, LR: 0.010000\n",
      "[36, 100] loss: 1.849 | acc: 28.5%\n",
      "[36, 200] loss: 1.854 | acc: 28.2%\n",
      "[36, 300] loss: 1.867 | acc: 27.8%\n",
      "Test Epoch: 36, Loss: 1.7891, Acc: 31.32%\n",
      "Epoch 37, LR: 0.010000\n",
      "[37, 100] loss: 1.871 | acc: 28.1%\n",
      "[37, 200] loss: 1.865 | acc: 27.7%\n",
      "[37, 300] loss: 1.852 | acc: 27.7%\n",
      "Test Epoch: 37, Loss: 1.7720, Acc: 31.91%\n",
      "Epoch 38, LR: 0.010000\n",
      "[38, 100] loss: 1.878 | acc: 27.8%\n",
      "[38, 200] loss: 1.865 | acc: 28.0%\n",
      "[38, 300] loss: 1.864 | acc: 27.7%\n",
      "Test Epoch: 38, Loss: 1.7781, Acc: 31.63%\n",
      "Epoch 39, LR: 0.010000\n",
      "[39, 100] loss: 1.845 | acc: 28.9%\n",
      "[39, 200] loss: 1.861 | acc: 28.4%\n",
      "[39, 300] loss: 1.878 | acc: 27.8%\n",
      "Test Epoch: 39, Loss: 1.7478, Acc: 33.55%\n",
      "Epoch 40, LR: 0.010000\n",
      "[40, 100] loss: 1.947 | acc: 26.1%\n",
      "[40, 200] loss: 1.923 | acc: 26.0%\n",
      "[40, 300] loss: 1.870 | acc: 26.2%\n",
      "Test Epoch: 40, Loss: 1.7862, Acc: 31.64%\n",
      "Epoch 41, LR: 0.010000\n",
      "[41, 100] loss: 1.850 | acc: 27.8%\n",
      "[41, 200] loss: 1.850 | acc: 27.9%\n",
      "[41, 300] loss: 1.872 | acc: 27.6%\n",
      "Test Epoch: 41, Loss: 1.8630, Acc: 29.32%\n",
      "Epoch 42, LR: 0.010000\n",
      "[42, 100] loss: 1.848 | acc: 28.3%\n",
      "[42, 200] loss: 1.841 | acc: 28.3%\n",
      "[42, 300] loss: 1.858 | acc: 28.0%\n",
      "Test Epoch: 42, Loss: 1.9174, Acc: 28.01%\n",
      "Epoch 43, LR: 0.010000\n",
      "[43, 100] loss: 1.855 | acc: 28.4%\n",
      "[43, 200] loss: 1.908 | acc: 27.6%\n",
      "[43, 300] loss: 1.864 | acc: 27.6%\n",
      "Test Epoch: 43, Loss: 1.8018, Acc: 30.96%\n",
      "Epoch 44, LR: 0.010000\n",
      "[44, 100] loss: 1.849 | acc: 28.4%\n",
      "[44, 200] loss: 1.874 | acc: 28.2%\n",
      "[44, 300] loss: 1.865 | acc: 27.8%\n",
      "Test Epoch: 44, Loss: 1.7804, Acc: 31.92%\n",
      "Epoch 45, LR: 0.010000\n",
      "[45, 100] loss: 1.857 | acc: 28.5%\n",
      "[45, 200] loss: 1.846 | acc: 28.6%\n",
      "[45, 300] loss: 1.853 | acc: 28.1%\n",
      "Test Epoch: 45, Loss: 1.7789, Acc: 32.47%\n",
      "Epoch 46, LR: 0.010000\n",
      "[46, 100] loss: 1.853 | acc: 28.1%\n",
      "[46, 200] loss: 1.847 | acc: 28.2%\n",
      "[46, 300] loss: 1.854 | acc: 28.1%\n",
      "Test Epoch: 46, Loss: 1.9146, Acc: 27.51%\n",
      "Epoch 47, LR: 0.010000\n",
      "[47, 100] loss: 1.849 | acc: 28.6%\n",
      "[47, 200] loss: 1.853 | acc: 28.4%\n",
      "[47, 300] loss: 1.850 | acc: 28.3%\n",
      "Test Epoch: 47, Loss: 1.7897, Acc: 32.11%\n",
      "Epoch 48, LR: 0.010000\n",
      "[48, 100] loss: 1.895 | acc: 27.2%\n",
      "[48, 200] loss: 1.860 | acc: 27.5%\n",
      "[48, 300] loss: 1.944 | acc: 26.9%\n",
      "Test Epoch: 48, Loss: 1.7879, Acc: 32.16%\n",
      "Epoch 49, LR: 0.010000\n",
      "[49, 100] loss: 1.851 | acc: 28.2%\n",
      "[49, 200] loss: 1.858 | acc: 28.4%\n",
      "[49, 300] loss: 1.856 | acc: 28.2%\n",
      "Test Epoch: 49, Loss: 1.9140, Acc: 27.88%\n",
      "Epoch 50, LR: 0.010000\n",
      "[50, 100] loss: 1.855 | acc: 28.3%\n",
      "[50, 200] loss: 1.847 | acc: 28.4%\n",
      "[50, 300] loss: 1.896 | acc: 27.8%\n",
      "Test Epoch: 50, Loss: 1.7922, Acc: 31.67%\n",
      "Best test accuracy: 33.55%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "initial_lr = 0.01#0.00001 #0.01\n",
    "weight_decay = 0.0005\n",
    "momentum = 0.9\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "phase_boundaries = [80, 120]  # Epochs where LR would normally be reduced by 10x\n",
    "\n",
    "# Define transformations for data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_dataset = torchvision.datasets.CIFAR10(\n",
    "#     root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "\n",
    "subset_dataset = torch.load('val_dataset.pth',weights_only=False)\n",
    "remaining_dataset = torch.load('test_dataset.pth',weights_only=False)\n",
    "val_loader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(remaining_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 10)  # Change output to 10 for CIFAR-10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu5(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = cnn().to(device)\n",
    "file_path = \"s_cnn.pth\"\n",
    "initial_weights=torch.load(file_path, weights_only=True)\n",
    "# Print(\"Model's initial weights\", initial_weights)\n",
    "model.load_state_dict(initial_weights)\n",
    "\n",
    "\n",
    "class AdamLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, initial_lr, phase_boundaries, last_epoch=-1):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.phase_boundaries = phase_boundaries\n",
    "\n",
    "        # Initialize step count and last_epoch for each parameter group before calling super().__init__()\n",
    "        for group in optimizer.param_groups:\n",
    "            group.setdefault('initial_step', 0)\n",
    "            group.setdefault('last_epoch', last_epoch)\n",
    "\n",
    "        super(AdamLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        epoch = self.last_epoch\n",
    "        current_phase = 0\n",
    "        for boundary in self.phase_boundaries:\n",
    "            if epoch >= boundary:\n",
    "                current_phase += 1\n",
    "\n",
    "        # Calculate base learning rate\n",
    "        current_lr = self.initial_lr * (0.1 ** current_phase)\n",
    "\n",
    "        # Return learning rates with Adam's bias correction\n",
    "        lrs = []\n",
    "        for group in self.optimizer.param_groups:\n",
    "            beta1 = group.get('betas', (0.9, 0.999))[0]\n",
    "\n",
    "            # Update the step counter if we're in a new epoch\n",
    "            if epoch > group.get('last_epoch', -1):\n",
    "                group['initial_step'] += len(train_loader)\n",
    "            group['last_epoch'] = epoch\n",
    "\n",
    "            bias_correction = 1 - beta1 ** group['initial_step']\n",
    "            corrected_lr = current_lr / bias_correction\n",
    "            lrs.append(corrected_lr)\n",
    "\n",
    "        return lrs\n",
    "\n",
    "\n",
    "# Loss and optimizer with Adam\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                      lr=initial_lr, \n",
    "                      weight_decay=weight_decay,\n",
    "                      betas=(0.9, 0.999))  # Default Adam betas\n",
    "\n",
    "scheduler = AdamLR(optimizer, initial_lr, phase_boundaries)\n",
    "\n",
    "# Training loop\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, LR: {scheduler.get_lr()[0]:.6f}')\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 99:\n",
    "            print(f'[{epoch+1}, {batch_idx+1}] loss: {running_loss/100:.3f} | acc: {100.*correct/total:.1f}%')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    acc = 100. * correct / total\n",
    "    print(f'Test Epoch: {epoch+1}, Loss: {test_loss/(batch_idx+1):.4f}, Acc: {acc:.2f}%')\n",
    "    return acc\n",
    "\n",
    "best_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    train(epoch)\n",
    "    acc = test(epoch)\n",
    "    \n",
    "    # Save best model\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "print(f'Best test accuracy: {best_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ea624-bb8f-4a87-96c1-9c6b9a401d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
