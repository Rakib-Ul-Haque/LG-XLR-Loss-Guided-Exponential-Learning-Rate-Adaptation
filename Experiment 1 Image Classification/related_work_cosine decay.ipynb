{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae4feed9-1eb0-4e4f-9583-ca6c96412795",
   "metadata": {},
   "source": [
    "<h5>This code is part of XLR. It implements cosine decay learning rate model training. Date: 01/09/2025</h5>\n",
    "\n",
    "<h5>Contact: rakibul.haque@utsa.edu</h5>\n",
    "\n",
    "<h5>Cite as: R. U. Haque and P. Markopoulos,\"XLR: A Universal Framework for Learning rate Adaptation via Exponential Range Exploration\", 2025</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14bea459-a295-4223-aefe-cff4c940855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from collections import Counter\n",
    "import random\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "import queue\n",
    "from collections import deque\n",
    "from torch import Tensor\n",
    "from typing import Type\n",
    "import torchvision.models as models\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44862c03-65dd-4c35-9b2b-059f2d2aa2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a3e617-b429-46e1-8e64-3b9cff62b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print(string, dictionary):\n",
    "    first_key = next(iter(dictionary))\n",
    "    first_value = dictionary[first_key]\n",
    "    print(f\"{string}:{first_key}: {first_value[0][0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b25f7285-4daf-4620-af4a-a0d85804aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 10)  # Change output to 10 for CIFAR-10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu5(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e510997e-4f47-4222-929c-8c4c3509becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    # Compute average loss and accuracy\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e63b2416-6bef-4377-b49c-15495f1de8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's initial weights:conv1.weight: tensor([[ 0.1329, -0.1625,  0.0656],\n",
      "        [ 0.0480,  0.0967,  0.0128],\n",
      "        [ 0.1696,  0.0517, -0.0104]], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model_centralized = cnn()\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_centralized.to(device)\n",
    "\n",
    "file_path = \"s_cnn.pth\"\n",
    "initial_weights=torch.load(file_path, weights_only=True)\n",
    "Print(\"Model's initial weights\", initial_weights)\n",
    "\n",
    "model_centralized.load_state_dict(initial_weights)\n",
    "\n",
    "# b_size=128\n",
    "# num_epochs = 50\n",
    "# le_rate=0.01\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "eta_min = 0.00001\n",
    "eta_max = 0.1\n",
    "rho_w = 10\n",
    "ntest = 10\n",
    "confidence = 0.99\n",
    "patience = 2\n",
    "\n",
    "opti=\"sgd\"\n",
    "# opti=\"adam\"\n",
    "# opti=\"adagrad\"\n",
    "# opti=\"adadelta\"\n",
    "# opti=\"rmdprop\"\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(model_centralized.parameters(), lr=0.01)\n",
    "\n",
    "if opti==\"sgd\":\n",
    "    optimizer = torch.optim.SGD(model_centralized.parameters(), lr=eta_min)\n",
    "elif opti==\"adam\":\n",
    "    optimizer = torch.optim.Adam(model_centralized.parameters(), lr=eta_min)\n",
    "elif opti==\"adagrad\":\n",
    "    optimizer = torch.optim.Adagrad(model_centralized.parameters(), lr=eta_min)\n",
    "elif opti==\"adadelta\":\n",
    "    optimizer = torch.optim.Adadelta(model_centralized.parameters(), lr=eta_min)\n",
    "elif opti==\"rmdprop\":\n",
    "    optimizer = torch.optim.RMSprop(model_centralized.parameters(), lr=eta_min)\n",
    "\n",
    "\n",
    "name=f\"==related_work_large_batchsize_{opti}_{batch_size}_{num_epochs}\"\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "learning_rate= []\n",
    "learning_rate.append(None)\n",
    "\n",
    "\n",
    "\n",
    "# DataLoader for batch processing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "subset_dataset = torch.load('val_dataset.pth',weights_only=False)\n",
    "remaining_dataset = torch.load('test_dataset.pth',weights_only=False)\n",
    "val_loader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(remaining_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "warmup_steps = int(rho_w * total_steps)\n",
    "gamma = (eta_max / eta_min) ** (1 / warmup_steps)\n",
    "\n",
    "\n",
    "# AutoWU Implementation\n",
    "lr = eta_min\n",
    "patience_flag = 0\n",
    "warmup_phase = True\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b3c148-e450-4e59-92e4-f63a68500e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_and_gradient(model, optimizer, inputs, labels):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def cosine_decay_lr(step, T, eta_min, eta_max):\n",
    "    return eta_min + 0.5 * (eta_max - eta_min) * (1 + np.cos(np.pi * step / T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "995c4493-5248-418a-97fe-9c5654d83af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#round zero\n",
    "train_loss, train_accuracy = evaluate(model_centralized, train_loader, criterion, device)\n",
    "train_losses.append(train_loss)\n",
    "train_accuracies.append(train_accuracy)\n",
    "\n",
    "val_loss, val_accuracy = evaluate(model_centralized, val_loader, criterion, device)\n",
    "val_losses.append(val_loss)\n",
    "val_accuracies.append(val_accuracy)\n",
    "\n",
    "test_loss, test_accuracy = evaluate(model_centralized, test_loader, criterion, device)\n",
    "test_losses.append(test_loss)\n",
    "test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "480ad7d3-a782-43e8-bb9e-a09b0194c740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train Loss: 2.3027, Train Accuracy: 9.99%\n",
      "Test Loss: 2.3027, Test Accuracy: 10.01%\n",
      "Epoch 2/50\n",
      "Train Loss: 2.3027, Train Accuracy: 10.00%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 3/50\n",
      "Train Loss: 2.3027, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 4/50\n",
      "Train Loss: 2.3027, Train Accuracy: 10.00%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 5/50\n",
      "Train Loss: 2.3027, Train Accuracy: 10.00%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 6/50\n",
      "Train Loss: 2.3027, Train Accuracy: 10.00%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 7/50\n",
      "Train Loss: 2.3027, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 8/50\n",
      "Train Loss: 2.3027, Train Accuracy: 10.00%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 9/50\n",
      "Train Loss: 2.3027, Train Accuracy: 10.00%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 10/50\n",
      "Train Loss: 2.3027, Train Accuracy: 10.02%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 11/50\n",
      "Train Loss: 2.3027, Train Accuracy: 10.02%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 12/50\n",
      "Train Loss: 2.3026, Train Accuracy: 9.99%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 13/50\n",
      "Train Loss: 2.3026, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 14/50\n",
      "Train Loss: 2.3026, Train Accuracy: 10.02%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 15/50\n",
      "Train Loss: 2.3026, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 16/50\n",
      "Train Loss: 2.3026, Train Accuracy: 9.99%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 17/50\n",
      "Train Loss: 2.3026, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 18/50\n",
      "Train Loss: 2.3026, Train Accuracy: 10.00%\n",
      "Test Loss: 2.3026, Test Accuracy: 10.01%\n",
      "Epoch 19/50\n",
      "Train Loss: 2.3026, Train Accuracy: 9.98%\n",
      "Test Loss: 2.3025, Test Accuracy: 10.01%\n",
      "Epoch 20/50\n",
      "Train Loss: 2.3026, Train Accuracy: 10.02%\n",
      "Test Loss: 2.3025, Test Accuracy: 10.01%\n",
      "Epoch 21/50\n",
      "Train Loss: 2.3026, Train Accuracy: 10.00%\n",
      "Test Loss: 2.3025, Test Accuracy: 10.01%\n",
      "Epoch 22/50\n",
      "Train Loss: 2.3026, Train Accuracy: 10.00%\n",
      "Test Loss: 2.3025, Test Accuracy: 10.01%\n",
      "Epoch 23/50\n",
      "Train Loss: 2.3026, Train Accuracy: 10.00%\n",
      "Test Loss: 2.3025, Test Accuracy: 10.01%\n",
      "Epoch 24/50\n",
      "Train Loss: 2.3026, Train Accuracy: 10.00%\n",
      "Test Loss: 2.3025, Test Accuracy: 10.01%\n",
      "Epoch 25/50\n",
      "Train Loss: 2.3026, Train Accuracy: 9.99%\n",
      "Test Loss: 2.3025, Test Accuracy: 10.01%\n",
      "Epoch 26/50\n",
      "Train Loss: 2.3026, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3025, Test Accuracy: 10.01%\n",
      "Epoch 27/50\n",
      "Train Loss: 2.3026, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3025, Test Accuracy: 10.01%\n",
      "Epoch 28/50\n",
      "Train Loss: 2.3026, Train Accuracy: 10.00%\n",
      "Test Loss: 2.3025, Test Accuracy: 10.01%\n",
      "Epoch 29/50\n",
      "Train Loss: 2.3025, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3025, Test Accuracy: 10.01%\n",
      "Epoch 30/50\n",
      "Train Loss: 2.3025, Train Accuracy: 10.00%\n",
      "Test Loss: 2.3025, Test Accuracy: 10.01%\n",
      "Epoch 31/50\n",
      "Train Loss: 2.3026, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3025, Test Accuracy: 10.01%\n",
      "Epoch 32/50\n",
      "Train Loss: 2.3025, Train Accuracy: 10.00%\n",
      "Test Loss: 2.3024, Test Accuracy: 10.01%\n",
      "Epoch 33/50\n",
      "Train Loss: 2.3025, Train Accuracy: 10.02%\n",
      "Test Loss: 2.3024, Test Accuracy: 10.01%\n",
      "Epoch 34/50\n",
      "Train Loss: 2.3025, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3024, Test Accuracy: 10.01%\n",
      "Epoch 35/50\n",
      "Train Loss: 2.3025, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3024, Test Accuracy: 10.01%\n",
      "Epoch 36/50\n",
      "Train Loss: 2.3025, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3024, Test Accuracy: 10.01%\n",
      "Epoch 37/50\n",
      "Train Loss: 2.3025, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3024, Test Accuracy: 10.01%\n",
      "Epoch 38/50\n",
      "Train Loss: 2.3025, Train Accuracy: 10.02%\n",
      "Test Loss: 2.3024, Test Accuracy: 10.01%\n",
      "Epoch 39/50\n",
      "Train Loss: 2.3025, Train Accuracy: 10.02%\n",
      "Test Loss: 2.3024, Test Accuracy: 10.01%\n",
      "Epoch 40/50\n",
      "Train Loss: 2.3024, Train Accuracy: 10.02%\n",
      "Test Loss: 2.3024, Test Accuracy: 10.01%\n",
      "Epoch 41/50\n",
      "Train Loss: 2.3025, Train Accuracy: 9.99%\n",
      "Test Loss: 2.3024, Test Accuracy: 10.01%\n",
      "Epoch 42/50\n",
      "Train Loss: 2.3024, Train Accuracy: 9.99%\n",
      "Test Loss: 2.3024, Test Accuracy: 10.01%\n",
      "Epoch 43/50\n",
      "Train Loss: 2.3024, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3023, Test Accuracy: 10.01%\n",
      "Epoch 44/50\n",
      "Train Loss: 2.3024, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3023, Test Accuracy: 10.01%\n",
      "Epoch 45/50\n",
      "Train Loss: 2.3024, Train Accuracy: 10.03%\n",
      "Test Loss: 2.3023, Test Accuracy: 10.01%\n",
      "Epoch 46/50\n",
      "Train Loss: 2.3024, Train Accuracy: 10.01%\n",
      "Test Loss: 2.3023, Test Accuracy: 10.01%\n",
      "Epoch 47/50\n",
      "Train Loss: 2.3024, Train Accuracy: 10.00%\n",
      "Test Loss: 2.3023, Test Accuracy: 10.01%\n",
      "Epoch 48/50\n",
      "Train Loss: 2.3024, Train Accuracy: 10.02%\n",
      "Test Loss: 2.3023, Test Accuracy: 10.01%\n",
      "Epoch 49/50\n",
      "Train Loss: 2.3024, Train Accuracy: 10.04%\n",
      "Test Loss: 2.3023, Test Accuracy: 10.01%\n",
      "Epoch 50/50\n",
      "Train Loss: 2.3024, Train Accuracy: 10.03%\n",
      "Test Loss: 2.3023, Test Accuracy: 10.01%\n"
     ]
    }
   ],
   "source": [
    "step_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    running_corrects = 0  # Tracks the number of correct predictions\n",
    "    running_loss=0\n",
    "    total_samples = 0  # Tracks the total number of samples processed\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Increment the step counter\n",
    "        step_counter += 1\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_centralized(inputs)\n",
    "        _, preds = torch.max(outputs, 1)  # Get predictions\n",
    "        total_samples += labels.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        \n",
    "        # Update learning rate during warmup phase\n",
    "        if warmup_phase:\n",
    "            lr = lr * gamma if step_counter < warmup_steps else lr\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            \n",
    "            # End of warmup phase checks\n",
    "            if step_counter >= warmup_steps:\n",
    "                warmup_phase = False\n",
    "                # Implement Gaussian Process and confidence checks as needed\n",
    "        else:\n",
    "            # Cosine decay after warmup\n",
    "            lr = cosine_decay_lr(step_counter - warmup_steps, total_steps - warmup_steps, eta_min, eta_max)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "    \n",
    "    # Calculate training accuracy for the epoch\n",
    "    train_accuracy = running_corrects / total_samples * 100\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    learning_rate.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    #===========================================================================================\n",
    "    # Evaluate the model and get test loss and accuracy\n",
    "    val_loss, val_accuracy = evaluate(model_centralized, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    test_loss, test_accuracy = evaluate(model_centralized, test_loader, criterion, device)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2571882-7f44-497c-a511-be6b310a7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensor data to CPU and then to Python lists if necessary\n",
    "train_losses = [loss.item() if torch.is_tensor(loss) else loss for loss in train_losses]\n",
    "train_accuracies = [accuracy.item() if torch.is_tensor(accuracy) else accuracy for accuracy in train_accuracies]\n",
    "val_losses = [loss.item() if torch.is_tensor(loss) else loss for loss in val_losses]\n",
    "val_accuracies = [accuracy.item() if torch.is_tensor(accuracy) else accuracy for accuracy in val_accuracies]\n",
    "test_losses = [loss.item() if torch.is_tensor(loss) else loss for loss in test_losses]\n",
    "test_accuracies = [accuracy.item() if torch.is_tensor(accuracy) else accuracy for accuracy in test_accuracies]\n",
    "learning_rate = [lr.item() if torch.is_tensor(lr) else lr for lr in learning_rate]\n",
    "\n",
    "# # Ensure the directory exists\n",
    "os.makedirs(name, exist_ok=True)\n",
    "# Prepare data for DataFrame\n",
    "data = {\n",
    "    \"Train Losses\": train_losses,\n",
    "    \"Train Accuracies\": train_accuracies,\n",
    "    \"Validation Losses\": val_losses,\n",
    "    \"Validation Accuracies\": val_accuracies,\n",
    "    \"Test Losses\": test_losses,\n",
    "    \"Test Accuracies\": test_accuracies,\n",
    "    \"Learning Rate\": learning_rate,\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "file_path = f'{name}/{name}_metrics.csv'\n",
    "df.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
